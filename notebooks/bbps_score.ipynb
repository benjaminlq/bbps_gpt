{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "from openai.types.chat.completion_create_params import ResponseFormat\n",
    "import base64\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from typing import Optional, Literal, List, Tuple, Union, Literal\n",
    "from tqdm import tqdm\n",
    "\n",
    "import base64\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \"..\"\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "ARTIFACT_DIR = os.path.join(MAIN_DIR, \"artifacts\")\n",
    "\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "    \n",
    "openai.api_key = api_keys[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the image\n",
    "def encode_image(image_path: str, resize: Optional[Union[int, Tuple[int, int], Literal[\"auto\"]]] = None):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        img_64_str = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    if resize:\n",
    "        img_data = base64.b64decode(img_64_str)\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        h, w = img.size\n",
    "        if isinstance(resize, int):\n",
    "            resize = (resize, resize)\n",
    "        elif resize == \"auto\":\n",
    "            if h < 512 and w < 512:\n",
    "                resize = (h, w)\n",
    "            elif h > w:\n",
    "                resize = (512, int(w / (h/512)))\n",
    "            else:\n",
    "                resize = (int(h / (w/512)), 512)\n",
    "                \n",
    "        resized_img = img.resize(resize)\n",
    "        \n",
    "        # Save the resized image to a buffer\n",
    "        buffer = io.BytesIO()\n",
    "        resized_img.save(buffer, format=\"PNG\")\n",
    "        buffer.seek(0)\n",
    "        \n",
    "        # Encode the resized image to base64\n",
    "        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    else:\n",
    "        return img_64_str\n",
    "    \n",
    "def generate_img_url(image_path: str, resize: Optional[Union[int, Tuple[int, int], Literal[\"auto\"]]] = None):\n",
    "    encoded_image = encode_image(image_path, resize=resize)\n",
    "    image_url = f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "    return image_url\n",
    "\n",
    "class TokenCounter:\n",
    "    def __init__(self):\n",
    "        self.token_counter = {}\n",
    "        self.cost_counter = {\"prompt\": 0, \"completion\": 0, \"total\": 0}\n",
    "        self.token_cost = {\n",
    "            \"gpt-4-1106-preview\": {\"prompt_tokens\": 0.01, \"completion_tokens\": 0.03},\n",
    "            \"gpt-4-vision-preview\": {\"prompt_tokens\": 0.01, \"completion_tokens\": 0.03},\n",
    "            \"gpt-4-1106-vision-preview\": {\"prompt_tokens\": 0.01, \"completion_tokens\": 0.03},\n",
    "            \"gpt-3.5-turbo-1106\": {\"prompt_tokens\": 0.001, \"completion_tokens\": 0.002},\n",
    "        }\n",
    "    \n",
    "    def update(self, response: ChatCompletion, verbose: bool = False):\n",
    "        model_name = response.model\n",
    "        prompt_tokens = response.usage.prompt_tokens\n",
    "        completion_tokens = response.usage.completion_tokens\n",
    "        if verbose:\n",
    "            print(\"Latest API Call on {} model usage: Prompt Tokens - {}, Completion Tokens - {}\"\n",
    "                .format(model_name, prompt_tokens, completion_tokens))\n",
    "        if model_name in self.token_counter:\n",
    "            self.token_counter[model_name][\"prompt_tokens\"] += prompt_tokens\n",
    "            self.token_counter[model_name][\"completion_tokens\"] += completion_tokens\n",
    "        else:\n",
    "            self.token_counter[model_name] = {\"prompt_tokens\": prompt_tokens, \"completion_tokens\": completion_tokens}\n",
    "        self.update_cost(prompt_tokens, completion_tokens, model_name, verbose=verbose)\n",
    "            \n",
    "    def update_cost(self, prompt_tokens: int, completion_tokens: int, model_name: str = \"gpt-4-vision-preview\",\n",
    "                    verbose: bool = False):\n",
    "        prompt_unit_cost = self.token_cost[model_name][\"prompt_tokens\"]\n",
    "        completion_unit_cost = self.token_cost[model_name][\"completion_tokens\"]\n",
    "        prompt_cost = prompt_tokens / 1000 * prompt_unit_cost\n",
    "        completion_cost = completion_tokens / 1000 * completion_unit_cost\n",
    "        if verbose:\n",
    "            print(\"Latest API Call on {} model. Cost: Prompt Cost - {}, Completion Cost - {}\"\n",
    "                .format(model_name, prompt_cost, completion_cost))\n",
    "        self.cost_counter[\"prompt\"] += prompt_cost\n",
    "        self.cost_counter[\"completion\"] += completion_cost\n",
    "        self.cost_counter[\"total\"] += (prompt_cost + completion_cost)\n",
    "            \n",
    "    def reset(self):\n",
    "        self.token_counter = {}\n",
    "        self.cost_counter = {\"prompt\": 0, \"completion\": 0, \"total\": 0}\n",
    "        \n",
    "def extract_score(response_str: str, client: OpenAI, token_counter: Optional[TokenCounter] = None):\n",
    "    system_prompt = \"\"\"You are given a response containing the BBPS grading.\n",
    "    Extract the BBPS score given in the response. If the score is not available, return empty string.\n",
    "    Your output should be a JSON dictionary with key \"Score\" and value containing integer score. \n",
    "    Example Output: {\"Score\": 0}, {\"Score\": \"\"}\n",
    "    \"\"\"\n",
    "    user_prompt = f\"Response: {response_str}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]},\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_prompt}]}\n",
    "        ],\n",
    "        max_tokens=128, temperature=0,\n",
    "        response_format=ResponseFormat(type=\"json_object\")\n",
    "    )\n",
    "    if token_counter:\n",
    "        token_counter.update(response)\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content, strict=False)\n",
    "\n",
    "def calculate_token_img(w: int, h: int, quality: Literal[\"low\", \"high\"] = \"high\"):\n",
    "    if quality == \"low\":\n",
    "        return 85\n",
    "    \n",
    "    if w > 2048 and h > 2048:\n",
    "        if w <= h:\n",
    "            w = int(w / (h / 2048))\n",
    "            h = 2048\n",
    "        else:\n",
    "            h = int(h / (w/2048))\n",
    "            w = 2048\n",
    "    elif w > 2048:\n",
    "        h = int(h / (w/2048))\n",
    "        w = 2048\n",
    "    elif h > 2048:\n",
    "        w = int(w / (h / 2048))\n",
    "        h = 2048    \n",
    "    if w > 512 and h > 512:\n",
    "        if w >= h:\n",
    "            w = int(w / (h/768))\n",
    "            h = 768\n",
    "        else:\n",
    "            h = int(h / (w/768))\n",
    "            w = 768\n",
    "        \n",
    "    no_of_tiles = math.ceil(w/512) * math.ceil(h/512)\n",
    "    return 170 * no_of_tiles + 85\n",
    "\n",
    "def save_checkpoint(\n",
    "    ckpt_folder: str,\n",
    "    img_paths: List[str],\n",
    "    gpt_raw_answers: List[str],\n",
    "    gpt_scores: List[str]\n",
    "):\n",
    "    content = dict(\n",
    "        img_paths=img_paths,\n",
    "        gpt_raw_answers=gpt_raw_answers,\n",
    "        gpt_scores=gpt_scores\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(ckpt_folder):\n",
    "        os.makedirs(ckpt_folder, exist_ok=True)\n",
    "    with open(os.path.join(ckpt_folder, \"ckpt.json\"), \"w\") as f:\n",
    "        json.dump(content, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = TokenCounter()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nerthus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(os.path.join(DATA_DIR, \"nerthus\", \"eval_set\"))\n",
    "ids = [int(filename.split(\"_\")[1]) for filename in filenames]\n",
    "gt_scores = [int(filename.split(\"_\")[3].split(\"-\")[0]) for filename in filenames]\n",
    "save_folder = os.path.join(ARTIFACT_DIR, \"nerthus\")\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Text Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "=====\n",
    "TASK:\n",
    "You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Perform the following step:\n",
    "1. Analyse the given image and identify the degree of stool and residual staining and whether mucosa of colon can be seen well.\n",
    "2. Based on the GRADING CRITERIA and EXAMPLES given, return the BBPS grade for the given image. Can be one of [0, 1, 2, 3]\n",
    "=====\n",
    "GRADING CRITERIA: Use the following BBPS Grading Criteria to determine the Grade of the given bowel image.\n",
    "Grade 0: Unprepared colon segment with mucosa not seen due to solid stool that cannot be cleared\n",
    "Grade 1: Portion of mucosa of the colon segment seen, but other areas of the colon segment not well seen due to staining, residual stool and/or opaque liquid\n",
    "Grade 2: Minor amount of residual staining, small fragments of stool and/or opaque liquid, but mucosa of colon segment seen well\n",
    "Grade 3: Entire mucosa of colon segment seen well with no residual staining, small fragments of stool or opaque liquid. The wording of the scale was finalized after incorporating feedback from three colleagues experienced in colonoscopy.\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "query_prompt=\"Analyse this bowel image and return the BBPS score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:01<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SEED = 2023\n",
    "model_name = \"gpt-4-vision-preview\"\n",
    "text_responses = []\n",
    "text_gpt_scores = []\n",
    "\n",
    "for filename in tqdm(filenames, total=len(filenames)):\n",
    "    query_img_path = os.path.join(MAIN_DIR, \"data\", \"nerthus\", \"eval_set\", filename)\n",
    "    query_img_url = generate_img_url(query_img_path)\n",
    "    gptv_response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": task_system_prompt}\n",
    "                    ],\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": query_prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": query_img_url, \"detail\": \"low\"},\n",
    "                    ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512,\n",
    "        temperature=0,\n",
    "        seed=SEED\n",
    "    )\n",
    "    token_counter.update(gptv_response)\n",
    "    response_str = gptv_response.choices[0].message.content\n",
    "    text_responses.append(response_str)\n",
    "    score_dict = extract_score(response_str, client, token_counter)\n",
    "    text_gpt_scores.append(score_dict[\"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "=====\n",
    "TASK:\n",
    "You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Perform the following step:\n",
    "1. Analyse the given image and identify the degree of stool and residual staining and whether mucosa of colon can be seen well.\n",
    "2. Based on the EXAMPLES given, return the BBPS grade for the given image. Can be one of [0, 1, 2, 3]\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "example_system_prompt=\"\"\"\n",
    "=====\n",
    "EXAMPLE:\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "query_prompt=\"Analyse this bowel image and return the BBPS score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_path_1 = os.path.join(MAIN_DIR, \"data\", \"samples\", \"example_1.JPG\")\n",
    "sample_img_path_2 = os.path.join(MAIN_DIR, \"data\", \"samples\", \"example_2.JPG\")\n",
    "\n",
    "sample_img_url_1 = generate_img_url(sample_img_path_1)\n",
    "sample_img_url_2 = generate_img_url(sample_img_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:36<00:00,  6.08s/it]\n"
     ]
    }
   ],
   "source": [
    "SEED = 2023\n",
    "model_name = \"gpt-4-vision-preview\"\n",
    "fs_responses = []\n",
    "fs_gpt_scores = []\n",
    "\n",
    "for filename in tqdm(filenames, total=len(filenames)):\n",
    "    query_img_path = os.path.join(MAIN_DIR, \"data\", \"nerthus\", \"eval_set\", filename)\n",
    "    query_img_url = generate_img_url(query_img_path)\n",
    "    gptv_response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": task_system_prompt}\n",
    "                    ],\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": example_system_prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": sample_img_url_1, \"detail\": \"low\"}\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": example_system_prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": sample_img_url_2, \"detail\": \"low\"}\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": query_prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": query_img_url, \"detail\": \"low\"},\n",
    "                    ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512,\n",
    "        temperature=0,\n",
    "        seed=SEED\n",
    "    )\n",
    "    token_counter.update(gptv_response)\n",
    "    response_str = gptv_response.choices[0].message.content\n",
    "    fs_responses.append(response_str)\n",
    "    score_dict = extract_score(response_str, client, token_counter)\n",
    "    fs_gpt_scores.append(score_dict[\"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot With Text Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "=====\n",
    "TASK:\n",
    "You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Perform the following step:\n",
    "1. Analyse the given image and identify the degree of stool and residual staining and whether mucosa of colon can be seen well.\n",
    "2. Based on the GRADING CRITERIA and EXAMPLES given, return the BBPS grade for the given image. Can be one of [0, 1, 2, 3]\n",
    "=====\n",
    "GRADING CRITERIA: Use the following BBPS Grading Criteria to determine the Grade of the given bowel image.\n",
    "Grade 0: Unprepared colon segment with mucosa not seen due to solid stool that cannot be cleared\n",
    "Grade 1: Portion of mucosa of the colon segment seen, but other areas of the colon segment not well seen due to staining, residual stool and/or opaque liquid\n",
    "Grade 2: Minor amount of residual staining, small fragments of stool and/or opaque liquid, but mucosa of colon segment seen well\n",
    "Grade 3: Entire mucosa of colon segment seen well with no residual staining, small fragments of stool or opaque liquid. The wording of the scale was finalized after incorporating feedback from three colleagues experienced in colonoscopy.\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "example_system_prompt=\"\"\"\n",
    "=====\n",
    "EXAMPLE:\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "query_prompt=\"Analyse this bowel image and return the BBPS score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_path_1 = os.path.join(MAIN_DIR, \"data\", \"samples\", \"example_1.JPG\")\n",
    "sample_img_path_2 = os.path.join(MAIN_DIR, \"data\", \"samples\", \"example_2.JPG\")\n",
    "\n",
    "sample_img_url_1 = generate_img_url(sample_img_path_1)\n",
    "sample_img_url_2 = generate_img_url(sample_img_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2023\n",
    "model_name = \"gpt-4-vision-preview\"\n",
    "fs_text_responses = []\n",
    "fs_text_gpt_scores = []\n",
    "\n",
    "# for filename in tqdm(filenames, total=len(filenames)):\n",
    "for filename in filenames:\n",
    "    query_img_path = os.path.join(MAIN_DIR, \"data\", \"nerthus\", \"eval_set\", filename)\n",
    "    query_img_url = generate_img_url(query_img_path)\n",
    "    gptv_response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": task_system_prompt}\n",
    "                    ],\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": example_system_prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": sample_img_url_1, \"detail\": \"low\"}\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": example_system_prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": sample_img_url_2, \"detail\": \"low\"}\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": query_prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": query_img_url, \"detail\": \"low\"},\n",
    "                    ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512,\n",
    "        temperature=0,\n",
    "        seed=SEED\n",
    "    )\n",
    "    token_counter.update(gptv_response)\n",
    "    response_str = gptv_response.choices[0].message.content\n",
    "    fs_text_responses.append(response_str)\n",
    "    score_dict = extract_score(response_str, client, token_counter)\n",
    "    fs_text_gpt_scores.append(score_dict[\"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_json = []\n",
    "for filename, id, gt_score, text_raw_answer, text_score, fs_raw_answer, fs_score, fs_text_raw_answer, fs_text_score \\\n",
    "    in zip(filenames, ids, gt_scores, text_responses, text_gpt_scores, fs_responses, fs_gpt_scores, fs_text_responses, fs_text_gpt_scores):\n",
    "        exp_json.append(\n",
    "            {\n",
    "                \"filename\": filename,\n",
    "                \"id\": id,\n",
    "                \"gt_score\": gt_score,\n",
    "                \"text_raw_answer\": text_raw_answer,\n",
    "                \"text_score\": text_score,\n",
    "                \"fs_raw_answer\": fs_raw_answer,\n",
    "                \"fs_score\": fs_score,\n",
    "                \"fs_text_raw_answer\": fs_text_raw_answer,\n",
    "                \"fs_text_score\": fs_text_score\n",
    "            }\n",
    "        )\n",
    "        \n",
    "with open(os.path.join(save_folder, \"result.json\"), \"w\") as f:\n",
    "    json.dump(exp_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dict = {\n",
    "    \"filename\": filenames,\n",
    "    \"id\": ids,\n",
    "    \"gt_score\": gt_scores,\n",
    "    \"text_raw_answer\": text_responses,\n",
    "    \"text_score\": text_gpt_scores,\n",
    "    \"fs_raw_answer\": fs_responses,\n",
    "    \"fs_score\": fs_gpt_scores,\n",
    "    \"fs_text_raw_answer\": fs_text_responses,\n",
    "    \"fs_text_score\": fs_text_gpt_scores\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(pd_dict)\n",
    "df.to_csv(os.path.join(save_folder, \"result.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Kvasir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_score_folder = \"bbps-0-1\"\n",
    "data_folder = os.path.join(DATA_DIR, \"hyper-kvasir\")\n",
    "\n",
    "with open(os.path.join(data_folder, \"testcases.txt\"), 'r') as fp:\n",
    "    data = fp.read()\n",
    "    all_file_paths = data.split(\"\\n\") \n",
    "\n",
    "gt_scores = [0 if (path.split(\"/\")[3] == low_score_folder) else 1 for path in all_file_paths]\n",
    "filenames = [path.split(\"/\")[-1] for path in all_file_paths]\n",
    "\n",
    "assert len(gt_scores) == len(filenames)\n",
    "\n",
    "save_folder = os.path.join(ARTIFACT_DIR, \"hyper-kvasir\")\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_path_1 = os.path.join(MAIN_DIR, \"data\", \"samples\", \"example_1.JPG\")\n",
    "sample_img_path_2 = os.path.join(MAIN_DIR, \"data\", \"samples\", \"example_2.JPG\")\n",
    "\n",
    "sample_img_url_1 = generate_img_url(sample_img_path_1)\n",
    "sample_img_url_2 = generate_img_url(sample_img_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens needed for sample 1: 765\n",
      "Tokens needed for sample 2: 765\n",
      "Total number of query image tokens required: 152490\n",
      "Total number of few-shot tokens reuqired: 2744820\n",
      "Total number of image tokens: 2897310\n",
      "Total image tokens cost: 28.9731\n"
     ]
    }
   ],
   "source": [
    "# # Token Usage Estimation\n",
    "# tokens_used = []\n",
    "# ws, hs = [], []\n",
    "# sample_img_1 = Image.open(sample_img_path_1)\n",
    "# sample_img_2 = Image.open(sample_img_path_2)\n",
    "# w1, h1 = sample_img_1.size\n",
    "# w2, h2 = sample_img_2.size\n",
    "# sample_img_tokens_1 = calculate_token_img(w1, h1, \"high\")\n",
    "# sample_img_tokens_2 = calculate_token_img(w2, h2, \"high\")\n",
    "# print(\"Tokens needed for sample 1:\", sample_img_tokens_1)\n",
    "# print(\"Tokens needed for sample 2:\", sample_img_tokens_2)\n",
    "\n",
    "# for img_path in all_file_paths:\n",
    "#     image = Image.open(img_path)\n",
    "#     w, h = image.size\n",
    "#     ws.append(w)\n",
    "#     hs.append(h)\n",
    "#     img_tokens = calculate_token_img(w, h, \"low\")\n",
    "#     tokens_used.append(img_tokens)\n",
    "    \n",
    "# query_tokens = sum(tokens_used)\n",
    "# sample_tokens = (sample_img_tokens_1 + sample_img_tokens_2) * len(all_file_paths)\n",
    "# total_tokens = query_tokens + sample_tokens\n",
    "    \n",
    "# print(\"Total number of query image tokens required:\", query_tokens)\n",
    "# print(\"Total number of few-shot tokens reuqired:\", sample_tokens)\n",
    "# print(\"Total number of image tokens:\", total_tokens)\n",
    "# print(\"Total image tokens cost:\", total_tokens / 1000 * 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot With Text Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "=====\n",
    "TASK:\n",
    "You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Perform the following step:\n",
    "1. Analyse the given image and identify the degree of stool and residual staining and whether mucosa of colon can be seen well.\n",
    "2. Based on the GRADING CRITERIA and EXAMPLES given, return the BBPS grade for the given image. Can be one of [0, 1, 2, 3]\n",
    "=====\n",
    "GRADING CRITERIA: Use the following BBPS Grading Criteria to determine the Grade of the given bowel image.\n",
    "Grade 0: Unprepared colon segment with mucosa not seen due to solid stool that cannot be cleared\n",
    "Grade 1: Portion of mucosa of the colon segment seen, but other areas of the colon segment not well seen due to staining, residual stool and/or opaque liquid\n",
    "Grade 2: Minor amount of residual staining, small fragments of stool and/or opaque liquid, but mucosa of colon segment seen well\n",
    "Grade 3: Entire mucosa of colon segment seen well with no residual staining, small fragments of stool or opaque liquid. The wording of the scale was finalized after incorporating feedback from three colleagues experienced in colonoscopy.\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "# task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "# If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "# =====\n",
    "# TASK:\n",
    "# You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "# The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Perform the following step:\n",
    "# 1. Analyse the given image and identify the degree of stool and residual staining and whether mucosa of colon can be seen well.\n",
    "# 2. Based on the EXAMPLES given, return the BBPS grade for the given image. Can be one of [0, 1, 2, 3]\n",
    "# =====\n",
    "# \"\"\"\n",
    "\n",
    "# task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "# If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "# =====\n",
    "# TASK:\n",
    "# You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "# The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Based on the GRADING CRITERIA and EXAMPLES given, return the BBPS grade for the given image.\n",
    "# =====\n",
    "# OUTPUT INSTRUCTION:\n",
    "# Return your output as a single BBPS score which is one of [0, 1, 2, 3].\n",
    "# =====\n",
    "# GRADING CRITERIA: Use the following BBPS Grading Criteria to determine the Grade of the given bowel image.\n",
    "# Grade 0: Unprepared colon segment with mucosa not seen due to solid stool that cannot be cleared\n",
    "# Grade 1: Portion of mucosa of the colon segment seen, but other areas of the colon segment not well seen due to staining, residual stool and/or opaque liquid\n",
    "# Grade 2: Minor amount of residual staining, small fragments of stool and/or opaque liquid, but mucosa of colon segment seen well\n",
    "# Grade 3: Entire mucosa of colon segment seen well with no residual staining, small fragments of stool or opaque liquid. The wording of the scale was finalized after incorporating feedback from three colleagues experienced in colonoscopy.\n",
    "# =====\n",
    "# \"\"\"\n",
    "\n",
    "example_system_prompt=\"\"\"\n",
    "=====\n",
    "EXAMPLE:\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "query_prompt=\"Analyse this bowel image and return the BBPS score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2023\n",
    "model_name = \"gpt-4-vision-preview\"\n",
    "checkpoint = os.path.join(ARTIFACT_DIR, \"hyper-kvasir\", \"checkpoint\", \"1050\", \"ckpt.json\")\n",
    "# checkpoint = None\n",
    "resize = \"auto\" \n",
    "# resize = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no = 100\n",
    "total_sample_no = len(all_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint:\n",
    "    with open(checkpoint, \"r\") as f:\n",
    "        ckpt_content = json.load(f)\n",
    "    fs_text_responses = ckpt_content[\"gpt_raw_answers\"]\n",
    "    fs_text_gpt_scores = ckpt_content[\"gpt_scores\"]\n",
    "    start = int(checkpoint.split(\"/\")[-2])\n",
    "    end = start + sample_no if sample_no else total_sample_no\n",
    "\n",
    "else:\n",
    "    fs_text_responses = []\n",
    "    fs_text_gpt_scores = []\n",
    "    start = 0\n",
    "    end = start + sample_no if sample_no else total_sample_no\n",
    "\n",
    "print(f\"Run testcase from sample idx {start} to sample idx {end}\")\n",
    "for idx, query_img_path in enumerate(tqdm(all_file_paths[start:end], total=len(all_file_paths[start:end])),\n",
    "                                     start=start):\n",
    "\n",
    "# for idx, query_img_path in enumerate(all_file_paths[start:end], start=start):\n",
    "    query_img_url = generate_img_url(query_img_path, resize=resize)\n",
    "    gptv_response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": task_system_prompt}\n",
    "                    ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": example_system_prompt},\n",
    "                    {\"type\": \"image_url\",\n",
    "                     \"image_url\": {\"url\": sample_img_url_1, \"detail\": \"high\"}},\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": example_system_prompt},\n",
    "                    {\"type\": \"image_url\",\n",
    "                     \"image_url\": {\"url\": sample_img_url_2, \"detail\": \"high\"}},\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": query_prompt},\n",
    "                    {\"type\": \"image_url\",\n",
    "                     \"image_url\": {\"url\": query_img_url, \"detail\": \"high\"}},\n",
    "                    ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512,\n",
    "        temperature=0,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    token_counter.update(gptv_response)\n",
    "    response_str = gptv_response.choices[0].message.content\n",
    "    fs_text_responses.append(response_str)\n",
    "    score_dict = extract_score(response_str, client, token_counter)\n",
    "    fs_text_gpt_scores.append(score_dict[\"Score\"])\n",
    "    \n",
    "    if (idx + 1) % 20 == 0:\n",
    "        ckpt_folder = os.path.join(ARTIFACT_DIR, \"hyper-kvasir\", \"checkpoint\", str(idx+1))\n",
    "        save_checkpoint(ckpt_folder, all_file_paths, fs_text_responses, fs_text_gpt_scores)\n",
    "        print(\"Successfully saved checkpoint at folder:\", ckpt_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved checkpoint at folder: ../artifacts/hyper-kvasir/checkpoint/1140\n"
     ]
    }
   ],
   "source": [
    "ckpt_folder = os.path.join(ARTIFACT_DIR, \"hyper-kvasir\", \"checkpoint\", str(idx+1))\n",
    "save_checkpoint(ckpt_folder, all_file_paths, fs_text_responses, fs_text_gpt_scores)\n",
    "print(\"Successfully saved checkpoint at folder:\", ckpt_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Accuracy: 82.018\n"
     ]
    }
   ],
   "source": [
    "exp_json = []\n",
    "for filename, gt_score, fs_text_raw_answer, fs_text_score \\\n",
    "    in zip(filenames[:end], gt_scores[:end], fs_text_responses, fs_text_gpt_scores):\n",
    "        exp_json.append(\n",
    "            {\n",
    "                \"filename\": filename,\n",
    "                \"gt_score\": gt_score,\n",
    "                \"fs_text_raw_answer\": fs_text_raw_answer,\n",
    "                \"fs_text_score\": fs_text_score\n",
    "            }\n",
    "        )\n",
    "        \n",
    "with open(os.path.join(save_folder, f\"result_{0}-{end}.json\"), \"w\") as f:\n",
    "    json.dump(exp_json, f)\n",
    "    \n",
    "pd_dict = {\n",
    "    \"filename\": filenames[:end],\n",
    "    \"gt_score\": gt_scores[:end],\n",
    "    \"fs_text_raw_answer\": fs_text_responses,\n",
    "    \"fs_text_score\": fs_text_gpt_scores\n",
    "}\n",
    "\n",
    "def classify_text_score(score):\n",
    "    if score == 0 or score == 1:\n",
    "        return 0\n",
    "    elif score == 2 or score == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "df = pd.DataFrame(pd_dict)\n",
    "df[\"gpt_classification\"] = df[\"fs_text_score\"].apply(lambda x: classify_text_score(x)).astype(int)\n",
    "df[\"match\"] = df[\"gt_score\"] == df[\"gpt_classification\"]\n",
    "df.to_csv(os.path.join(save_folder, f\"result_{0}-{end}.csv\"))\n",
    "\n",
    "accuracy = df[\"match\"].sum() / df[\"match\"].count()\n",
    "print(\"GPT Accuracy:\", round(accuracy * 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gt_score\n",
       "1    426\n",
       "0    244\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"gt_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fs_text_score\n",
       "3    509\n",
       "2    179\n",
       "1    117\n",
       "0     53\n",
       "      12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"fs_text_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt_classification\n",
       " 1    767\n",
       " 0    189\n",
       "-1     14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"gpt_classification\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6399108138238573"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(gt_scores)/len(gt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gt_score  match\n",
       "0         True     187\n",
       "          False    162\n",
       "1         True     611\n",
       "          False     10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"gt_score\")[\"match\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
