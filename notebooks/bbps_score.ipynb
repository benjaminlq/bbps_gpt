{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "from openai.types.chat.completion_create_params import ResponseFormat\n",
    "import base64\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from typing import Optional, Literal, List, Tuple, Union, Literal\n",
    "from tqdm import tqdm\n",
    "\n",
    "import base64\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "ARTIFACT_DIR = os.path.join(MAIN_DIR, \"artifacts\")\n",
    "\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "    \n",
    "openai.api_key = api_keys[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the image\n",
    "def encode_image(image_path: str, resize: Optional[Union[int, Tuple[int, int], Literal[\"auto\"]]] = None):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        img_64_str = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    if resize:\n",
    "        img_data = base64.b64decode(img_64_str)\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        h, w = img.size\n",
    "        if isinstance(resize, int):\n",
    "            resize = (resize, resize)\n",
    "        elif resize == \"auto\":\n",
    "            if h < 512 and w < 512:\n",
    "                resize = (h, w)\n",
    "            elif h > w:\n",
    "                resize = (512, int(w / (h/512)))\n",
    "            else:\n",
    "                resize = (int(h / (w/512)), 512)\n",
    "                \n",
    "        resized_img = img.resize(resize)\n",
    "        \n",
    "        # Save the resized image to a buffer\n",
    "        buffer = io.BytesIO()\n",
    "        resized_img.save(buffer, format=\"PNG\")\n",
    "        buffer.seek(0)\n",
    "        \n",
    "        # Encode the resized image to base64\n",
    "        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    else:\n",
    "        return img_64_str\n",
    "    \n",
    "def generate_img_url(image_path: str, resize: Optional[Union[int, Tuple[int, int], Literal[\"auto\"]]] = None):\n",
    "    encoded_image = encode_image(image_path, resize=resize)\n",
    "    image_url = f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "    return image_url\n",
    "\n",
    "class TokenCounter:\n",
    "    def __init__(self):\n",
    "        self.token_counter = {}\n",
    "        self.cost_counter = {\"prompt\": 0, \"completion\": 0, \"total\": 0}\n",
    "        self.token_cost = {\n",
    "            \"gpt-4-1106-preview\": {\"prompt_tokens\": 0.01, \"completion_tokens\": 0.03},\n",
    "            \"gpt-4-vision-preview\": {\"prompt_tokens\": 0.01, \"completion_tokens\": 0.03},\n",
    "            \"gpt-4-1106-vision-preview\": {\"prompt_tokens\": 0.01, \"completion_tokens\": 0.03},\n",
    "            \"gpt-3.5-turbo-1106\": {\"prompt_tokens\": 0.001, \"completion_tokens\": 0.002},\n",
    "        }\n",
    "    \n",
    "    def update(self, response: ChatCompletion, verbose: bool = False):\n",
    "        model_name = response.model\n",
    "        prompt_tokens = response.usage.prompt_tokens\n",
    "        completion_tokens = response.usage.completion_tokens\n",
    "        if verbose:\n",
    "            print(\"Latest API Call on {} model usage: Prompt Tokens - {}, Completion Tokens - {}\"\n",
    "                .format(model_name, prompt_tokens, completion_tokens))\n",
    "        if model_name in self.token_counter:\n",
    "            self.token_counter[model_name][\"prompt_tokens\"] += prompt_tokens\n",
    "            self.token_counter[model_name][\"completion_tokens\"] += completion_tokens\n",
    "        else:\n",
    "            self.token_counter[model_name] = {\"prompt_tokens\": prompt_tokens, \"completion_tokens\": completion_tokens}\n",
    "        self.update_cost(prompt_tokens, completion_tokens, model_name, verbose=verbose)\n",
    "            \n",
    "    def update_cost(self, prompt_tokens: int, completion_tokens: int, model_name: str = \"gpt-4-vision-preview\",\n",
    "                    verbose: bool = False):\n",
    "        prompt_unit_cost = self.token_cost[model_name][\"prompt_tokens\"]\n",
    "        completion_unit_cost = self.token_cost[model_name][\"completion_tokens\"]\n",
    "        prompt_cost = prompt_tokens / 1000 * prompt_unit_cost\n",
    "        completion_cost = completion_tokens / 1000 * completion_unit_cost\n",
    "        if verbose:\n",
    "            print(\"Latest API Call on {} model. Cost: Prompt Cost - {}, Completion Cost - {}\"\n",
    "                .format(model_name, prompt_cost, completion_cost))\n",
    "        self.cost_counter[\"prompt\"] += prompt_cost\n",
    "        self.cost_counter[\"completion\"] += completion_cost\n",
    "        self.cost_counter[\"total\"] += (prompt_cost + completion_cost)\n",
    "            \n",
    "    def reset(self):\n",
    "        self.token_counter = {}\n",
    "        self.cost_counter = {\"prompt\": 0, \"completion\": 0, \"total\": 0}\n",
    "        \n",
    "def extract_score(response_str: str, client: OpenAI, token_counter: Optional[TokenCounter] = None):\n",
    "    system_prompt = \"\"\"You are given a response containing the BBPS grading.\n",
    "    Extract the BBPS score given in the response. If the score is not available, return empty string.\n",
    "    Your output should be a JSON dictionary with key \"Score\" and value containing integer score. \n",
    "    Example Output: {\"Score\": 0}, {\"Score\": \"\"}\n",
    "    \"\"\"\n",
    "    user_prompt = f\"Response: {response_str}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]},\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_prompt}]}\n",
    "        ],\n",
    "        max_tokens=128, temperature=0,\n",
    "        response_format=ResponseFormat(type=\"json_object\")\n",
    "    )\n",
    "    if token_counter:\n",
    "        token_counter.update(response)\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content, strict=False)\n",
    "\n",
    "def calculate_token_img(w: int, h: int, quality: Literal[\"low\", \"high\"] = \"high\"):\n",
    "    if quality == \"low\":\n",
    "        return 85\n",
    "    \n",
    "    if w > 2048 and h > 2048:\n",
    "        if w <= h:\n",
    "            w = int(w / (h / 2048))\n",
    "            h = 2048\n",
    "        else:\n",
    "            h = int(h / (w/2048))\n",
    "            w = 2048\n",
    "    elif w > 2048:\n",
    "        h = int(h / (w/2048))\n",
    "        w = 2048\n",
    "    elif h > 2048:\n",
    "        w = int(w / (h / 2048))\n",
    "        h = 2048    \n",
    "    if w > 512 and h > 512:\n",
    "        if w >= h:\n",
    "            w = int(w / (h/768))\n",
    "            h = 768\n",
    "        else:\n",
    "            h = int(h / (w/768))\n",
    "            w = 768\n",
    "        \n",
    "    no_of_tiles = math.ceil(w/512) * math.ceil(h/512)\n",
    "    return 170 * no_of_tiles + 85\n",
    "\n",
    "def save_checkpoint(\n",
    "    ckpt_folder: str,\n",
    "    img_paths: List[str],\n",
    "    gpt_raw_answers: List[str],\n",
    "    gpt_scores: List[str]\n",
    "):\n",
    "    content = dict(\n",
    "        img_paths=img_paths,\n",
    "        gpt_raw_answers=gpt_raw_answers,\n",
    "        gpt_scores=gpt_scores\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(ckpt_folder):\n",
    "        os.makedirs(ckpt_folder, exist_ok=True)\n",
    "    with open(os.path.join(ckpt_folder, \"ckpt.json\"), \"w\") as f:\n",
    "        json.dump(content, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = TokenCounter()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nerthus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(os.path.join(DATA_DIR, \"nerthus\", \"eval_set\"))\n",
    "ids = [int(filename.split(\"_\")[1]) for filename in filenames]\n",
    "gt_scores = [int(filename.split(\"_\")[3].split(\"-\")[0]) for filename in filenames]\n",
    "save_folder = os.path.join(ARTIFACT_DIR, \"nerthus\")\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Text Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "=====\n",
    "TASK:\n",
    "You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Perform the following step:\n",
    "1. Analyse the given image and identify the degree of stool and residual staining and whether mucosa of colon can be seen well.\n",
    "2. Based on the GRADING CRITERIA and EXAMPLES given, return the BBPS grade for the given image. Can be one of [0, 1, 2, 3]\n",
    "=====\n",
    "GRADING CRITERIA: Use the following BBPS Grading Criteria to determine the Grade of the given bowel image.\n",
    "Grade 0: Unprepared colon segment with mucosa not seen due to solid stool that cannot be cleared\n",
    "Grade 1: Portion of mucosa of the colon segment seen, but other areas of the colon segment not well seen due to staining, residual stool and/or opaque liquid\n",
    "Grade 2: Minor amount of residual staining, small fragments of stool and/or opaque liquid, but mucosa of colon segment seen well\n",
    "Grade 3: Entire mucosa of colon segment seen well with no residual staining, small fragments of stool or opaque liquid. The wording of the scale was finalized after incorporating feedback from three colleagues experienced in colonoscopy.\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "query_prompt=\"Analyse this bowel image and return the BBPS score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:01<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SEED = 2023\n",
    "model_name = \"gpt-4-vision-preview\"\n",
    "text_responses = []\n",
    "text_gpt_scores = []\n",
    "\n",
    "for filename in tqdm(filenames, total=len(filenames)):\n",
    "    query_img_path = os.path.join(MAIN_DIR, \"data\", \"nerthus\", \"eval_set\", filename)\n",
    "    query_img_url = generate_img_url(query_img_path)\n",
    "    gptv_response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": task_system_prompt}\n",
    "                    ],\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": query_prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": query_img_url, \"detail\": \"low\"},\n",
    "                    ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512,\n",
    "        temperature=0,\n",
    "        seed=SEED\n",
    "    )\n",
    "    token_counter.update(gptv_response)\n",
    "    response_str = gptv_response.choices[0].message.content\n",
    "    text_responses.append(response_str)\n",
    "    score_dict = extract_score(response_str, client, token_counter)\n",
    "    text_gpt_scores.append(score_dict[\"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "=====\n",
    "TASK:\n",
    "You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Perform the following step:\n",
    "1. Analyse the given image and identify the degree of stool and residual staining and whether mucosa of colon can be seen well.\n",
    "2. Based on the EXAMPLES given, return the BBPS grade for the given image. Can be one of [0, 1, 2, 3]\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "example_system_prompt=\"\"\"\n",
    "=====\n",
    "EXAMPLE:\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "query_prompt=\"Analyse this bowel image and return the BBPS score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_path_1 = os.path.join(MAIN_DIR, \"data\", \"samples\", \"example_1.JPG\")\n",
    "sample_img_path_2 = os.path.join(MAIN_DIR, \"data\", \"samples\", \"example_2.JPG\")\n",
    "\n",
    "sample_img_url_1 = generate_img_url(sample_img_path_1)\n",
    "sample_img_url_2 = generate_img_url(sample_img_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:36<00:00,  6.08s/it]\n"
     ]
    }
   ],
   "source": [
    "SEED = 2023\n",
    "model_name = \"gpt-4-vision-preview\"\n",
    "fs_responses = []\n",
    "fs_gpt_scores = []\n",
    "\n",
    "for filename in tqdm(filenames, total=len(filenames)):\n",
    "    query_img_path = os.path.join(MAIN_DIR, \"data\", \"nerthus\", \"eval_set\", filename)\n",
    "    query_img_url = generate_img_url(query_img_path)\n",
    "    gptv_response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": task_system_prompt}\n",
    "                    ],\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": example_system_prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": sample_img_url_1, \"detail\": \"low\"}\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": example_system_prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": sample_img_url_2, \"detail\": \"low\"}\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": query_prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": query_img_url, \"detail\": \"low\"},\n",
    "                    ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512,\n",
    "        temperature=0,\n",
    "        seed=SEED\n",
    "    )\n",
    "    token_counter.update(gptv_response)\n",
    "    response_str = gptv_response.choices[0].message.content\n",
    "    fs_responses.append(response_str)\n",
    "    score_dict = extract_score(response_str, client, token_counter)\n",
    "    fs_gpt_scores.append(score_dict[\"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot With Text Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "=====\n",
    "TASK:\n",
    "You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Perform the following step:\n",
    "1. Analyse the given image and identify the degree of stool and residual staining and whether mucosa of colon can be seen well.\n",
    "2. Based on the GRADING CRITERIA and EXAMPLES given, return the BBPS grade for the given image. Can be one of [0, 1, 2, 3]\n",
    "=====\n",
    "GRADING CRITERIA: Use the following BBPS Grading Criteria to determine the Grade of the given bowel image.\n",
    "Grade 0: Unprepared colon segment with mucosa not seen due to solid stool that cannot be cleared\n",
    "Grade 1: Portion of mucosa of the colon segment seen, but other areas of the colon segment not well seen due to staining, residual stool and/or opaque liquid\n",
    "Grade 2: Minor amount of residual staining, small fragments of stool and/or opaque liquid, but mucosa of colon segment seen well\n",
    "Grade 3: Entire mucosa of colon segment seen well with no residual staining, small fragments of stool or opaque liquid. The wording of the scale was finalized after incorporating feedback from three colleagues experienced in colonoscopy.\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "example_system_prompt=\"\"\"\n",
    "=====\n",
    "EXAMPLE:\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "query_prompt=\"Analyse this bowel image and return the BBPS score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_path_1 = os.path.join(MAIN_DIR, \"data\", \"samples\", \"example_1.JPG\")\n",
    "sample_img_path_2 = os.path.join(MAIN_DIR, \"data\", \"samples\", \"example_2.JPG\")\n",
    "\n",
    "sample_img_url_1 = generate_img_url(sample_img_path_1)\n",
    "sample_img_url_2 = generate_img_url(sample_img_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2023\n",
    "model_name = \"gpt-4-vision-preview\"\n",
    "fs_text_responses = []\n",
    "fs_text_gpt_scores = []\n",
    "\n",
    "# for filename in tqdm(filenames, total=len(filenames)):\n",
    "for filename in filenames:\n",
    "    query_img_path = os.path.join(MAIN_DIR, \"data\", \"nerthus\", \"eval_set\", filename)\n",
    "    query_img_url = generate_img_url(query_img_path)\n",
    "    gptv_response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": task_system_prompt}\n",
    "                    ],\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": example_system_prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": sample_img_url_1, \"detail\": \"low\"}\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": example_system_prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": sample_img_url_2, \"detail\": \"low\"}\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": query_prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": query_img_url, \"detail\": \"low\"},\n",
    "                    ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512,\n",
    "        temperature=0,\n",
    "        seed=SEED\n",
    "    )\n",
    "    token_counter.update(gptv_response)\n",
    "    response_str = gptv_response.choices[0].message.content\n",
    "    fs_text_responses.append(response_str)\n",
    "    score_dict = extract_score(response_str, client, token_counter)\n",
    "    fs_text_gpt_scores.append(score_dict[\"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_json = []\n",
    "for filename, id, gt_score, text_raw_answer, text_score, fs_raw_answer, fs_score, fs_text_raw_answer, fs_text_score \\\n",
    "    in zip(filenames, ids, gt_scores, text_responses, text_gpt_scores, fs_responses, fs_gpt_scores, fs_text_responses, fs_text_gpt_scores):\n",
    "        exp_json.append(\n",
    "            {\n",
    "                \"filename\": filename,\n",
    "                \"id\": id,\n",
    "                \"gt_score\": gt_score,\n",
    "                \"text_raw_answer\": text_raw_answer,\n",
    "                \"text_score\": text_score,\n",
    "                \"fs_raw_answer\": fs_raw_answer,\n",
    "                \"fs_score\": fs_score,\n",
    "                \"fs_text_raw_answer\": fs_text_raw_answer,\n",
    "                \"fs_text_score\": fs_text_score\n",
    "            }\n",
    "        )\n",
    "        \n",
    "with open(os.path.join(save_folder, \"result.json\"), \"w\") as f:\n",
    "    json.dump(exp_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dict = {\n",
    "    \"filename\": filenames,\n",
    "    \"id\": ids,\n",
    "    \"gt_score\": gt_scores,\n",
    "    \"text_raw_answer\": text_responses,\n",
    "    \"text_score\": text_gpt_scores,\n",
    "    \"fs_raw_answer\": fs_responses,\n",
    "    \"fs_score\": fs_gpt_scores,\n",
    "    \"fs_text_raw_answer\": fs_text_responses,\n",
    "    \"fs_text_score\": fs_text_gpt_scores\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(pd_dict)\n",
    "df.to_csv(os.path.join(save_folder, \"result.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Kvasir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_score_folder = \"bbps-0-1\"\n",
    "data_folder = os.path.join(DATA_DIR, \"hyper-kvasir\")\n",
    "\n",
    "with open(os.path.join(data_folder, \"testcases.txt\"), 'r') as fp:\n",
    "    data = fp.read()\n",
    "    all_file_paths = data.split(\"\\n\") \n",
    "\n",
    "gt_scores = [0 if (path.split(\"/\")[3] == low_score_folder) else 1 for path in all_file_paths]\n",
    "filenames = [path.split(\"/\")[-1] for path in all_file_paths]\n",
    "\n",
    "assert len(gt_scores) == len(filenames)\n",
    "\n",
    "save_folder = os.path.join(ARTIFACT_DIR, \"hyper-kvasir\")\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_path_1 = os.path.join(MAIN_DIR, \"data\", \"samples\", \"example_1.JPG\")\n",
    "sample_img_path_2 = os.path.join(MAIN_DIR, \"data\", \"samples\", \"example_2.JPG\")\n",
    "\n",
    "sample_img_url_1 = generate_img_url(sample_img_path_1)\n",
    "sample_img_url_2 = generate_img_url(sample_img_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens needed for sample 1: 765\n",
      "Tokens needed for sample 2: 765\n",
      "Total number of query image tokens required: 152490\n",
      "Total number of few-shot tokens reuqired: 2744820\n",
      "Total number of image tokens: 2897310\n",
      "Total image tokens cost: 28.9731\n"
     ]
    }
   ],
   "source": [
    "# # Token Usage Estimation\n",
    "# tokens_used = []\n",
    "# ws, hs = [], []\n",
    "# sample_img_1 = Image.open(sample_img_path_1)\n",
    "# sample_img_2 = Image.open(sample_img_path_2)\n",
    "# w1, h1 = sample_img_1.size\n",
    "# w2, h2 = sample_img_2.size\n",
    "# sample_img_tokens_1 = calculate_token_img(w1, h1, \"high\")\n",
    "# sample_img_tokens_2 = calculate_token_img(w2, h2, \"high\")\n",
    "# print(\"Tokens needed for sample 1:\", sample_img_tokens_1)\n",
    "# print(\"Tokens needed for sample 2:\", sample_img_tokens_2)\n",
    "\n",
    "# for img_path in all_file_paths:\n",
    "#     image = Image.open(img_path)\n",
    "#     w, h = image.size\n",
    "#     ws.append(w)\n",
    "#     hs.append(h)\n",
    "#     img_tokens = calculate_token_img(w, h, \"low\")\n",
    "#     tokens_used.append(img_tokens)\n",
    "    \n",
    "# query_tokens = sum(tokens_used)\n",
    "# sample_tokens = (sample_img_tokens_1 + sample_img_tokens_2) * len(all_file_paths)\n",
    "# total_tokens = query_tokens + sample_tokens\n",
    "    \n",
    "# print(\"Total number of query image tokens required:\", query_tokens)\n",
    "# print(\"Total number of few-shot tokens reuqired:\", sample_tokens)\n",
    "# print(\"Total number of image tokens:\", total_tokens)\n",
    "# print(\"Total image tokens cost:\", total_tokens / 1000 * 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot With Text Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "# If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "# =====\n",
    "# TASK:\n",
    "# You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "# The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Perform the following step:\n",
    "# 1. Analyse the given image and identify the degree of stool and residual staining and whether mucosa of colon can be seen well.\n",
    "# 2. Based on the GRADING CRITERIA and EXAMPLES given, return the BBPS grade for the given image. Can be one of [0, 1, 2, 3]\n",
    "# =====\n",
    "# GRADING CRITERIA: Use the following BBPS Grading Criteria to determine the Grade of the given bowel image.\n",
    "# Grade 0: Unprepared colon segment with mucosa not seen due to solid stool that cannot be cleared\n",
    "# Grade 1: Portion of mucosa of the colon segment seen, but other areas of the colon segment not well seen due to staining, residual stool and/or opaque liquid\n",
    "# Grade 2: Minor amount of residual staining, small fragments of stool and/or opaque liquid, but mucosa of colon segment seen well\n",
    "# Grade 3: Entire mucosa of colon segment seen well with no residual staining, small fragments of stool or opaque liquid.\n",
    "# =====\n",
    "# \"\"\"\n",
    "\n",
    "task_system_prompt_reverse = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "=====\n",
    "TASK:\n",
    "You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Perform the following step:\n",
    "1. Analyse the given image and identify the degree of stool and residual staining and whether mucosa of colon can be seen well.\n",
    "2. Based on the GRADING CRITERIA and EXAMPLES given, return the BBPS grade for the given image. Can be one of [0, 1, 2, 3]\n",
    "=====\n",
    "GRADING CRITERIA: Use the following BBPS Grading Criteria to determine the Grade of the given bowel image.\n",
    "Grade 3: Entire mucosa of colon segment seen well with no residual staining, small fragments of stool or opaque liquid.\n",
    "Grade 2: Minor amount of residual staining, small fragments of stool and/or opaque liquid, but mucosa of colon segment seen well\n",
    "Grade 1: Portion of mucosa of the colon segment seen, but other areas of the colon segment not well seen due to staining, residual stool and/or opaque liquid\n",
    "Grade 0: Unprepared colon segment with mucosa not seen due to solid stool that cannot be cleared\n",
    "=====\n",
    "\"\"\"\n",
    "\n",
    "# task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "# If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "# =====\n",
    "# TASK:\n",
    "# You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "# The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Perform the following step:\n",
    "# 1. Analyse the given image and identify the degree of stool and residual staining and whether mucosa of colon can be seen well.\n",
    "# 2. Based on the EXAMPLES given, return the BBPS grade for the given image. Can be one of [0, 1, 2, 3]\n",
    "# =====\n",
    "# \"\"\"\n",
    "\n",
    "# task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "# If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "# =====\n",
    "# TASK:\n",
    "# You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "# The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Based on the GRADING CRITERIA and EXAMPLES given, return the BBPS grade for the given image.\n",
    "# =====\n",
    "# OUTPUT INSTRUCTION:\n",
    "# Return your output as a single BBPS score which is one of [0, 1, 2, 3].\n",
    "# =====\n",
    "# GRADING CRITERIA: Use the following BBPS Grading Criteria to determine the Grade of the given bowel image.\n",
    "# Grade 0: Unprepared colon segment with mucosa not seen due to solid stool that cannot be cleared\n",
    "# Grade 1: Portion of mucosa of the colon segment seen, but other areas of the colon segment not well seen due to staining, residual stool and/or opaque liquid\n",
    "# Grade 2: Minor amount of residual staining, small fragments of stool and/or opaque liquid, but mucosa of colon segment seen well\n",
    "# Grade 3: Entire mucosa of colon segment seen well with no residual staining, small fragments of stool or opaque liquid.\n",
    "# =====\n",
    "# \"\"\"\n",
    "\n",
    "# task_system_prompt = \"\"\"You are an expert endoscopist in charge of bowel preparation for colonoscopy.\n",
    "# If you don't know the answer, say 'I don't know' do not try to make up an answer.\n",
    "# =====\n",
    "# TASK:\n",
    "# You are given an image of a bowel after cleansing, your task is to assess the quality of the bowel preparation.\n",
    "# The grading should be performed using the standardized Boston-Bowel-Preparation-Scale (BBPS). Perform the following step:\n",
    "# 1. Analyse the given image and identify the degree of stool and residual staining and whether mucosa of colon can be seen well.\n",
    "# 2. Based on the EXAMPLES given, return the BBPS grade for the given image. Can be one of [0, 1, 2, 3]\n",
    "# =====\n",
    "# \"\"\"\n",
    "\n",
    "# example_system_prompt=\"\"\"\n",
    "# =====\n",
    "# EXAMPLE:\n",
    "# =====\n",
    "# \"\"\"\n",
    "\n",
    "query_prompt=\"Analyse this bowel image and return the BBPS score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2023\n",
    "model_name = \"gpt-4-vision-preview\"\n",
    "checkpoint = os.path.join(ARTIFACT_DIR, \"hyper-kvasir\", \"checkpoint\", \"60\", \"ckpt.json\")\n",
    "# checkpoint = None\n",
    "resize = \"auto\" \n",
    "# resize = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no = 40\n",
    "total_sample_no = len(all_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run testcase from sample idx 60 to sample idx 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [06:56<2:11:50, 208.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb Cell 30\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m response_str \u001b[39m=\u001b[39m gptv_response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m fs_text_responses\u001b[39m.\u001b[39mappend(response_str)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m score_dict \u001b[39m=\u001b[39m extract_score(response_str, client, token_counter)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m fs_text_gpt_scores\u001b[39m.\u001b[39mappend(score_dict[\u001b[39m\"\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mif\u001b[39;00m (idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m20\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb Cell 30\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m system_prompt \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mYou are given a response containing the BBPS grading.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mExtract the BBPS score given in the response. If the score is not available, return empty string.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mYour output should be a JSON dictionary with key \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m and value containing integer score. \u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39mExample Output: \u001b[39m\u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: 0}, \u001b[39m\u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m}\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m user_prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResponse: \u001b[39m\u001b[39m{\u001b[39;00mresponse_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo-1106\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m     messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: [{\u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m: system_prompt}]},\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: [{\u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m: user_prompt}]}\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m     response_format\u001b[39m=\u001b[39;49mResponseFormat(\u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mjson_object\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39mif\u001b[39;00m token_counter:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/bbps_gpt/notebooks/bbps_score.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m     token_counter\u001b[39m.\u001b[39mupdate(response)\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/openai/resources/chat/completions.py:556\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    514\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    555\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 556\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    557\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    558\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    559\u001b[0m             {\n\u001b[1;32m    560\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    561\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    562\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    563\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    564\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    565\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    566\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    567\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    568\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    569\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    570\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    571\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    572\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    573\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    574\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    575\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    576\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    577\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    578\u001b[0m             },\n\u001b[1;32m    579\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    580\u001b[0m         ),\n\u001b[1;32m    581\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    582\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    583\u001b[0m         ),\n\u001b[1;32m    584\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    585\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    586\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    587\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    835\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    836\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    837\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    838\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    839\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    840\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/openai/_base_client.py:858\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_request(request)\n\u001b[1;32m    857\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 858\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(request, auth\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_auth, stream\u001b[39m=\u001b[39;49mstream)\n\u001b[1;32m    859\u001b[0m     log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    860\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mHTTP Request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m, request\u001b[39m.\u001b[39mmethod, request\u001b[39m.\u001b[39murl, response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mreason_phrase\n\u001b[1;32m    861\u001b[0m     )\n\u001b[1;32m    862\u001b[0m     response\u001b[39m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpx/_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    893\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m    894\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m    895\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    896\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 901\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    902\u001b[0m     request,\n\u001b[1;32m    903\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    904\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    905\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    906\u001b[0m )\n\u001b[1;32m    907\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpx/_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    926\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(auth_flow)\n\u001b[1;32m    928\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    930\u001b[0m         request,\n\u001b[1;32m    931\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    932\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    933\u001b[0m     )\n\u001b[1;32m    934\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpx/_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    964\u001b[0m     hook(request)\n\u001b[0;32m--> 966\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpx/_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1002\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1004\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1006\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpx/_transports/default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    216\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    217\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 228\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    230\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    233\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    234\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    235\u001b[0m     stream\u001b[39m=\u001b[39mResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    236\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    237\u001b[0m )\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[39m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[39m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mresponse_closed\u001b[39m\u001b[39m\"\u001b[39m, logger, request) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreceive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_response_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    112\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    177\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    213\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv(max_bytes)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1259\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1256\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1257\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1258\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1259\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(buflen)\n\u001b[1;32m   1260\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m)\n\u001b[1;32m   1133\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m x:\n\u001b[1;32m   1134\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m SSL_ERROR_EOF \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if checkpoint:\n",
    "    with open(checkpoint, \"r\") as f:\n",
    "        ckpt_content = json.load(f)\n",
    "    fs_text_responses = ckpt_content[\"gpt_raw_answers\"]\n",
    "    fs_text_gpt_scores = ckpt_content[\"gpt_scores\"]\n",
    "    start = int(checkpoint.split(\"/\")[-2])\n",
    "    end = start + sample_no if sample_no else total_sample_no\n",
    "    if end > total_sample_no:\n",
    "        end = total_sample_no\n",
    "\n",
    "else:\n",
    "    fs_text_responses = []\n",
    "    fs_text_gpt_scores = []\n",
    "    start = 0\n",
    "    end = start + sample_no if sample_no else total_sample_no\n",
    "\n",
    "print(f\"Run testcase from sample idx {start} to sample idx {end}\")\n",
    "for idx, query_img_path in enumerate(tqdm(all_file_paths[start:end], total=len(all_file_paths[start:end])),\n",
    "                                     start=start):\n",
    "\n",
    "# for idx, query_img_path in enumerate(all_file_paths[start:end], start=start):\n",
    "    query_img_url = generate_img_url(query_img_path, resize=resize)\n",
    "    gptv_response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": task_system_prompt_reverse}\n",
    "                    ],\n",
    "            },\n",
    "            # {\n",
    "            #     \"role\": \"system\",\n",
    "            #     \"content\": [\n",
    "            #         {\"type\": \"text\", \"text\": example_system_prompt},\n",
    "            #         {\"type\": \"image_url\",\n",
    "            #          \"image_url\": {\"url\": sample_img_url_1, \"detail\": \"high\"}},\n",
    "            # ]\n",
    "            # },\n",
    "            # {\n",
    "            #     \"role\": \"system\",\n",
    "            #     \"content\": [\n",
    "            #         {\"type\": \"text\", \"text\": example_system_prompt},\n",
    "            #         {\"type\": \"image_url\",\n",
    "            #          \"image_url\": {\"url\": sample_img_url_2, \"detail\": \"high\"}},\n",
    "            # ]\n",
    "            # },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": query_prompt},\n",
    "                    {\"type\": \"image_url\",\n",
    "                     \"image_url\": {\"url\": query_img_url, \"detail\": \"high\"}},\n",
    "                    ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512,\n",
    "        temperature=0,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    token_counter.update(gptv_response)\n",
    "    response_str = gptv_response.choices[0].message.content\n",
    "    fs_text_responses.append(response_str)\n",
    "    score_dict = extract_score(response_str, client, token_counter)\n",
    "    fs_text_gpt_scores.append(score_dict[\"Score\"])\n",
    "    \n",
    "    if (idx + 1) % 20 == 0:\n",
    "        ckpt_folder = os.path.join(ARTIFACT_DIR, \"hyper-kvasir\", \"checkpoint\", str(idx+1))\n",
    "        save_checkpoint(ckpt_folder, all_file_paths, fs_text_responses, fs_text_gpt_scores)\n",
    "        print(\"Successfully saved checkpoint at folder:\", ckpt_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved checkpoint at folder: /mnt/c/Users/QUAN/Desktop/bbps_gpt/artifacts/hyper-kvasir/checkpoint/240\n"
     ]
    }
   ],
   "source": [
    "ckpt_folder = os.path.join(ARTIFACT_DIR, \"hyper-kvasir\", \"checkpoint\", str(idx+1))\n",
    "save_checkpoint(ckpt_folder, all_file_paths, fs_text_responses, fs_text_gpt_scores)\n",
    "print(\"Successfully saved checkpoint at folder:\", ckpt_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Accuracy: 40.0\n"
     ]
    }
   ],
   "source": [
    "exp_json = []\n",
    "for filename, gt_score, fs_text_raw_answer, fs_text_score \\\n",
    "    in zip(filenames[:end], gt_scores[:end], fs_text_responses, fs_text_gpt_scores):\n",
    "        exp_json.append(\n",
    "            {\n",
    "                \"filename\": filename,\n",
    "                \"gt_score\": gt_score,\n",
    "                \"fs_text_raw_answer\": fs_text_raw_answer,\n",
    "                \"fs_text_score\": fs_text_score\n",
    "            }\n",
    "        )\n",
    "        \n",
    "with open(os.path.join(save_folder, f\"result_{0}-{end}.json\"), \"w\") as f:\n",
    "    json.dump(exp_json, f)\n",
    "    \n",
    "pd_dict = {\n",
    "    \"filename\": filenames[:end],\n",
    "    \"gt_score\": gt_scores[:end],\n",
    "    \"fs_text_raw_answer\": fs_text_responses,\n",
    "    \"fs_text_score\": fs_text_gpt_scores\n",
    "}\n",
    "\n",
    "def classify_text_score(score):\n",
    "    if score == 0 or score == 1:\n",
    "        return 0\n",
    "    elif score == 2 or score == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "df = pd.DataFrame(pd_dict)\n",
    "df[\"gpt_classification\"] = df[\"fs_text_score\"].apply(lambda x: classify_text_score(x)).astype(int)\n",
    "df[\"match\"] = df[\"gt_score\"] == df[\"gpt_classification\"]\n",
    "df.to_csv(os.path.join(save_folder, f\"result_{0}-{end}.csv\"))\n",
    "\n",
    "accuracy = df[\"match\"].sum() / df[\"match\"].count()\n",
    "print(\"GPT Accuracy:\", round(accuracy * 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(ARTIFACT_DIR, \"hyper-kvasir\", \"run_1\", \"result_0-1794.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Accuracy: 83.055\n"
     ]
    }
   ],
   "source": [
    "accuracy = df[\"match\"].sum() / df[\"match\"].count()\n",
    "print(\"GPT Accuracy:\", round(accuracy * 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gt_score\n",
       "1    1148\n",
       "0     646\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"gt_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fs_text_score\n",
       "3.0    1058\n",
       "2.0     353\n",
       "1.0     260\n",
       "0.0     101\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"fs_text_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt_classification\n",
       " 1    1411\n",
       " 0     361\n",
       "-1      22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"gpt_classification\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(gt_scores)/len(gt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gt_score  match\n",
       "0         True      357\n",
       "          False     289\n",
       "1         True     1133\n",
       "          False      15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"gt_score\")[\"match\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt_classification  match\n",
       "-1                  False      22\n",
       " 0                  True      357\n",
       "                    False       4\n",
       " 1                  True     1133\n",
       "                    False     278\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"gpt_classification\")[\"match\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"gpt_classification\"] == -1][\"gt_score\"].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
