{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "ARTIFACT_DIR = os.path.join(MAIN_DIR, \"artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(score):\n",
    "    if score == 0 or score == 1:\n",
    "        return 0\n",
    "    elif score == 2 or score == 3:\n",
    "        return 1\n",
    "    elif score == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        ValueError(\"Invalid Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(DATA_DIR, \"hyper-kvasir\")\n",
    "\n",
    "with open(os.path.join(data_folder, \"testcases.txt\"), 'r') as fp:\n",
    "    data = fp.read()\n",
    "    all_file_paths = data.split(\"\\n\") \n",
    "\n",
    "filenames = [path.split(\"/\")[-1] for path in all_file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fs_text_raw_answer</th>\n",
       "      <th>gpt_score</th>\n",
       "      <th>gt_score</th>\n",
       "      <th>gt_class</th>\n",
       "      <th>gpt_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f69bfb02-30c2-477c-905f-4c219dba30b1.jpg</td>\n",
       "      <td>Based on the image provided, the mucosa of the...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21ab075e-3ac3-4e8f-a455-6ca78dc5a248.jpg</td>\n",
       "      <td>Based on the image provided, the mucosa of the...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>be4dff9c-3f5d-40c2-8628-ee83c597b653.jpg</td>\n",
       "      <td>I'm sorry, but I cannot provide assistance wit...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a70e990c-9ae8-44bc-8e04-92071ee88039.jpg</td>\n",
       "      <td>Based on the image provided, the mucosa of the...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c6aae080-89f2-46e7-8fa9-266d57309b9c.jpg</td>\n",
       "      <td>Based on the image provided, the mucosa of the...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   filename  \\\n",
       "0  f69bfb02-30c2-477c-905f-4c219dba30b1.jpg   \n",
       "1  21ab075e-3ac3-4e8f-a455-6ca78dc5a248.jpg   \n",
       "2  be4dff9c-3f5d-40c2-8628-ee83c597b653.jpg   \n",
       "3  a70e990c-9ae8-44bc-8e04-92071ee88039.jpg   \n",
       "4  c6aae080-89f2-46e7-8fa9-266d57309b9c.jpg   \n",
       "\n",
       "                                  fs_text_raw_answer  gpt_score  gt_score  \\\n",
       "0  Based on the image provided, the mucosa of the...        0.0         1   \n",
       "1  Based on the image provided, the mucosa of the...        3.0         3   \n",
       "2  I'm sorry, but I cannot provide assistance wit...       -1.0         1   \n",
       "3  Based on the image provided, the mucosa of the...        3.0         3   \n",
       "4  Based on the image provided, the mucosa of the...        3.0         3   \n",
       "\n",
       "   gt_class  gpt_class  \n",
       "0         0          0  \n",
       "1         1          1  \n",
       "2         0         -1  \n",
       "3         1          1  \n",
       "4         1          1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.read_csv(\n",
    "    os.path.join(ARTIFACT_DIR, \"hyper-kvasir\", \"run_3\", \"result_0-1794.csv\"),\n",
    "    usecols = [\"filename\", \"fs_text_raw_answer\", \"fs_text_score\"]\n",
    ")\n",
    "\n",
    "result_df = result_df.rename(columns = {\"fs_text_score\": \"gpt_score\"})\n",
    "\n",
    "GT_DIR = os.path.join(DATA_DIR, \"hyper-kvasir\", \"ground_truths\")\n",
    "\n",
    "gt_dict = {}\n",
    "\n",
    "for gt_score in range(4):\n",
    "    img_folder = \"BBPS \" + str(gt_score)\n",
    "    img_files = os.listdir(os.path.join(GT_DIR, img_folder))\n",
    "    for img_file in img_files:\n",
    "        gt_dict[img_file] = gt_score\n",
    "\n",
    "result_df[\"gpt_score\"] = result_df[\"gpt_score\"].fillna(-1)\n",
    "result_df[\"gt_score\"] = [gt_dict[file] for file in result_df[\"filename\"]]\n",
    "\n",
    "result_df[\"gt_class\"] = result_df[\"gt_score\"].apply(lambda x: classify(x))\n",
    "result_df[\"gpt_class\"] = result_df[\"gpt_score\"].apply(lambda x: classify(x))\n",
    "\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, recall_score, classification_report, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score (0, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt_score\n",
       " 3.0    655\n",
       "-1.0    510\n",
       " 2.0    267\n",
       " 0.0    185\n",
       " 1.0    177\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[\"gpt_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0    0.00000   0.00000   0.00000         0\n",
      "         0.0    0.21081   0.31200   0.25161       125\n",
      "         1.0    0.68927   0.24254   0.35882       503\n",
      "         2.0    0.38202   0.54839   0.45033       186\n",
      "         3.0    0.84885   0.56735   0.68012       980\n",
      "\n",
      "    accuracy                        0.45652      1794\n",
      "   macro avg    0.42619   0.33406   0.34818      1794\n",
      "weighted avg    0.71125   0.45652   0.53636      1794\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = result_df[\"gpt_score\"]\n",
    "labels = result_df[\"gt_score\"]\n",
    "\n",
    "print(classification_report(labels, preds, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1    0.00000   0.00000   0.00000         0\n",
      "           0    0.91160   0.52548   0.66667       628\n",
      "           1    0.86117   0.68096   0.76054      1166\n",
      "\n",
      "    accuracy                        0.62653      1794\n",
      "   macro avg    0.59092   0.40215   0.47573      1794\n",
      "weighted avg    0.87882   0.62653   0.72768      1794\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/c/Users/QUAN/Desktop/bbps_gpt/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = result_df[\"gpt_class\"]\n",
    "labels = result_df[\"gt_class\"]\n",
    "\n",
    "print(classification_report(labels, preds, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37267751121057513\n"
     ]
    }
   ],
   "source": [
    "preds = result_df[\"gpt_class\"]\n",
    "labels = result_df[\"gt_class\"]\n",
    "\n",
    "print(cohen_kappa_score(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt_class\n",
       " 1    922\n",
       "-1    510\n",
       " 0    362\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[\"gpt_class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gt_score</th>\n",
       "      <th>gpt_score</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gt_score  gpt_score  count\n",
       "0          0       -1.0     51\n",
       "1          0        0.0     39\n",
       "2          0        1.0     25\n",
       "3          0        3.0      8\n",
       "4          0        2.0      2\n",
       "5          1        0.0    144\n",
       "6          1        1.0    122\n",
       "7          1       -1.0    119\n",
       "8          1        3.0     62\n",
       "9          1        2.0     56\n",
       "10         2        2.0    102\n",
       "11         2       -1.0     33\n",
       "12         2        3.0     29\n",
       "13         2        1.0     22\n",
       "14         3        3.0    556\n",
       "15         3       -1.0    307\n",
       "16         3        2.0    107\n",
       "17         3        1.0      8\n",
       "18         3        0.0      2"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.groupby(\"gt_score\")[\"gpt_score\"].value_counts().reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
